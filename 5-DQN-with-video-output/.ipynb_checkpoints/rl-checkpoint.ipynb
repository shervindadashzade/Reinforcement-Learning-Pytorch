{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ppYzva8dys1"
   },
   "source": [
    "## Preparing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "executionInfo": {
     "elapsed": 12391,
     "status": "ok",
     "timestamp": 1675768374325,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "WKHlnfnuWwhA",
    "outputId": "1cb222c5-1e77-48d5-e6ca-d7d4acf68fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gym==0.19.0 in /home/shervin/.local/lib/python3.8/site-packages (0.19.0)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle<1.7.0,>=1.2.0 in /home/shervin/.local/lib/python3.8/site-packages (from gym==0.19.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.18.0 in /home/shervin/.local/lib/python3.8/site-packages (from gym==0.19.0) (1.22.1)\n",
      "Requirement already satisfied: atari-py in /home/shervin/.local/lib/python3.8/site-packages (0.2.9)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from atari-py) (1.14.0)\n",
      "Requirement already satisfied: numpy in /home/shervin/.local/lib/python3.8/site-packages (from atari-py) (1.22.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gym==0.19.0\n",
    "!pip install atari-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60827,
     "status": "ok",
     "timestamp": 1675768435135,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "kS7QIBuuXOy7",
    "outputId": "609ed0d0-482d-49cc-ce16-8a9cdeabfe64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-07 11:12:52--  http://www.atarimania.com/roms/Roms.rar\n",
      "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
      "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19612325 (19M) [application/x-rar-compressed]\n",
      "Saving to: ‘Roms.rar’\n",
      "\n",
      "Roms.rar            100%[===================>]  18.70M   336KB/s    in 60s     \n",
      "\n",
      "2023-02-07 11:13:53 (319 KB/s) - ‘Roms.rar’ saved [19612325/19612325]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.atarimania.com/roms/Roms.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1675768435136,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "e7nh90HjXpnt"
   },
   "outputs": [],
   "source": [
    "!mkdir roms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYtVV149XTpr"
   },
   "outputs": [],
   "source": [
    "!unrar x Roms.rar roms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJsiwn3dYK2K"
   },
   "outputs": [],
   "source": [
    "!python -m atari_py.import_roms roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upQd4y3ld30u"
   },
   "source": [
    "## Making Environment and the DQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1675774354002,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "7vbLvvK0ZqCP"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import os.path\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675774355746,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "4UozmqeLfNax"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675774356481,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "oPqmllBZeeDh"
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "directory = './PongVideos/'\n",
    "env = gym.wrappers.Monitor(env, directory, force=True, video_callable=lambda episode_id: episode_id%20==0)\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1675774366404,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "7cLIif7Vldel",
    "outputId": "f4190894-b384-4805-a36f-8eb494c19096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "  print(torch.cuda.get_device_name('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675774374994,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "kWTZOSnOe4Sp"
   },
   "outputs": [],
   "source": [
    "###### PARAMS ######\n",
    "learning_rate = 0.0001\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 512\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "update_target_frequency = 2000\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 10000\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 18\n",
    "\n",
    "clip_error = True\n",
    "normalize_image = True\n",
    "\n",
    "file2save = 'pong_save.pth'\n",
    "save_model_frequency = 10000\n",
    "resume_previous_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXofRURj6U9e"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "new_state, reward, done, info = env.step(action)\n",
    "trans = (state, action, new_state,reward,done)\n",
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1675774242161,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "fuLpJK4a7SSN",
    "outputId": "a5f5f92f-7eb6-450f-fb66-7cd2802d9cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1675774391735,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "so6cArABfCKh"
   },
   "outputs": [],
   "source": [
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1675774394579,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "5149TxJyfUEp"
   },
   "outputs": [],
   "source": [
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675774394581,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "VuhT1cO0flY-"
   },
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    return torch.load(file2save)\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675774394582,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "5zpA3441fqFM"
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = cv2.resize(frame, (84,84), interpolation = cv2.INTER_AREA)\n",
    "    frame = np.expand_dims(frame,axis=2)\n",
    "    frame = frame.transpose((2,0,1))\n",
    "    frame = torch.from_numpy(frame)\n",
    "    frame = frame.to(device, dtype=torch.float32)\n",
    "    frame = frame.unsqueeze(1)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1072,
     "status": "ok",
     "timestamp": 1675774401270,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "cfjNo0-VfqHj"
   },
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards_total, alpha=0.6, color='red')\n",
    "    plt.savefig(\"Pong-results.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1675774401277,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "qgOiC0S0fqKJ"
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1675774401279,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "MNdJEG5dixYx"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.advantage1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.advantage2 = nn.Linear(hidden_layer, number_of_outputs)\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64,hidden_layer)\n",
    "        self.value2 = nn.Linear(hidden_layer,1)\n",
    "\n",
    "        #self.activation = nn.Tanh()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if normalize_image:\n",
    "            x = x / 255\n",
    "        \n",
    "        output_conv = self.conv1(x)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv2(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv3(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        \n",
    "        output_conv = output_conv.view(output_conv.size(0), -1) # flatten\n",
    "        \n",
    "        output_advantage = self.advantage1(output_conv)\n",
    "        output_advantage = self.activation(output_advantage)\n",
    "        output_advantage = self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value = self.value1(output_conv)\n",
    "        output_value = self.activation(output_value)\n",
    "        output_value = self.value2(output_value)\n",
    "        \n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "\n",
    "        return output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1675774401883,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "rXJg7P0njZzX"
   },
   "outputs": [],
   "source": [
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.number_of_frames = 0\n",
    "        \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print(\"Loading previously saved model ... \")\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1).item()\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = preprocess_frame(state)\n",
    "                action_from_nn = self.nn(state)\n",
    "                \n",
    "                action = torch.max(action_from_nn,1)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = [ preprocess_frame(frame) for frame in state ] \n",
    "        state = torch.cat(state)\n",
    "        \n",
    "        new_state = [ preprocess_frame(frame) for frame in new_state ] \n",
    "        new_state = torch.cat(new_state)\n",
    "\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "\n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        \n",
    "        \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.number_of_frames % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        if self.number_of_frames % save_model_frequency == 0:\n",
    "            save_model(self.nn)\n",
    "        \n",
    "        self.number_of_frames += 1\n",
    "        \n",
    "        #Q[state, action] = reward + gamma * torch.max(Q[new_state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4066,
     "status": "ok",
     "timestamp": 1675774410068,
     "user": {
      "displayName": "shervin dadashzade",
      "userId": "10378376594121928421"
     },
     "user_tz": -210
    },
    "id": "Vh4HXjc1j9av",
    "outputId": "8629d932-fedc-4c14-ed71-502665fd0a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously saved model ... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "rewards_total = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmCY5Pg8kRES",
    "outputId": "bcdf594f-4f04-448c-c588-bcb8a4aa8833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: -18.50, [last 100]: -2.03, [all]: -18.45                       \n",
      "epsilon: 0.14, frames_total: 19290\n",
      "Elapsed time:  00:07:16\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: -17.20, [last 100]: -3.75, [all]: -17.86                       \n",
      "epsilon: 0.02, frames_total: 42391\n",
      "Elapsed time:  00:16:02\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: -18.00, [last 100]: -5.55, [all]: -17.90                       \n",
      "epsilon: 0.01, frames_total: 66803\n",
      "Elapsed time:  00:25:13\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: -15.80, [last 100]: -7.13, [all]: -17.39                       \n",
      "epsilon: 0.01, frames_total: 94011\n",
      "Elapsed time:  00:35:28\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: -15.30, [last 100]: -8.66, [all]: -16.98                       \n",
      "epsilon: 0.01, frames_total: 123751\n",
      "Elapsed time:  00:46:42\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: -14.70, [last 100]: -10.13, [all]: -16.61                       \n",
      "epsilon: 0.01, frames_total: 154238\n",
      "Elapsed time:  00:58:05\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: -14.60, [last 100]: -11.59, [all]: -16.32                       \n",
      "epsilon: 0.01, frames_total: 186485\n",
      "Elapsed time:  01:10:02\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: -15.70, [last 100]: -13.16, [all]: -16.25                       \n",
      "epsilon: 0.01, frames_total: 215841\n",
      "Elapsed time:  01:20:54\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: -13.40, [last 100]: -14.50, [all]: -15.93                       \n",
      "epsilon: 0.01, frames_total: 247539\n",
      "Elapsed time:  01:32:34\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            rewards_total.append(score)\n",
    "            \n",
    "            mean_reward_100 = sum(rewards_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0 and i_episode > 0):\n",
    "                \n",
    "                plot_results()\n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(rewards_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(rewards_total)/len(rewards_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(rewards_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(rewards_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "\n",
    "\n",
    "env.close()\n",
    "env.env.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMVFclyCGKZPtA16YBK/bA9",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
